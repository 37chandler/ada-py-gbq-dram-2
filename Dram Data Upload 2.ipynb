{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "25b450e8",
   "metadata": {},
   "source": [
    "## Dram Data Upload\n",
    "\n",
    "<!-- \n",
    "\n",
    "Assignment Verison\n",
    "\n",
    "In this assignment, you'll upload Dram shop data to your GBQ account and run a couple of queries against it. The data for this assignment is in a .zip file called `dram-items.zip` on Moodle (though the underlying file will have a date attached to it). These are item-level reports exported from the point-of-sale system at the Dram shop.\n",
    "\n",
    "Your goals are to upload these to tables in a GBQ data set. Your tables should have the naming convention `dram_items_YYYYMM01` and each table should be a single month. (We're putting 01 in so that GBQ displays the tables in their \"shard\" format: `dram_items_(N)`.) \n",
    "\n",
    "I recommend extracting the zip file into the repository into a folder called `dram-items`. Make sure _not_ to commit any big data files to your repo.\n",
    "\n",
    "Make your data pipeline idempotent, which means you will be checking for the presence of your tables and, if they exist, deleting them before recreating them. \n",
    "\n",
    "--> \n",
    "\n",
    "In this exercise, you'll upload Dram shop data to your GBQ account and run a couple of queries against it. The data for this exercise is in a .zip file called `dram-items.zip` on Moodle (though the underlying file will have a date attached to it). These are item-level reports exported from the point-of-sale system at the Dram shop.\n",
    "\n",
    "Your goals are to upload these to tables in a GBQ data set. Your tables should have the naming convention `dram_items_YYYYMM01` and each table should be a single month. (We're putting 01 in so that GBQ displays the tables in their \"shard\" format: `dram_items_(N)`.) \n",
    "\n",
    "I recommend extracting the zip file into the repository into a folder called `dram-items`. Make sure _not_ to commit any big data files to your repo.\n",
    "\n",
    "Make your data pipeline idempotent, which means you will be checking for the presence of your tables and, if they exist, deleting them before recreating them. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d503d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import datetime \n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas_gbq\n",
    "import janitor\n",
    "\n",
    "# Do our imports for the code\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3813e659",
   "metadata": {},
   "source": [
    "### GBQ Set Up\n",
    "\n",
    "In this next section we connect to our GBQ project and list the data sets inside to test the connection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825198a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# These first two values will be different on your machine. \n",
    "service_path = \"/Users/chandler/Dropbox/Teaching/\"\n",
    "service_file = 'umt-msba-037daf11ee16.json' # change this to your authentication information  \n",
    "gbq_proj_id = 'umt-msba' # change this to your project. \n",
    "\n",
    "# And this should stay the same. \n",
    "private_key =service_path + service_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca232504",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we pass in our credentials so that Python has permission to access our project.\n",
    "credentials = service_account.Credentials.from_service_account_file(service_path + service_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4a6d11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And finally we establish our connection\n",
    "client = bigquery.Client(credentials = credentials, project=gbq_proj_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fa696d",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in client.list_datasets() : \n",
    "    print(item.full_dataset_id)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "08e1e74c",
   "metadata": {},
   "source": [
    "### Checking for and deleting monthly tables\n",
    "\n",
    "We'll get all the tables in our Dram data set that match our pattern, then delete them. We do not want to accidentally delete the item lookup table that we put in this data set in class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb93325a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ymd_pattern = re.compile(r\"\") # create a regex that matches our table pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fceca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_id = \"your_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6302800",
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = client.list_tables(dataset_id)  \n",
    "\n",
    "for table in tables:\n",
    "    \n",
    "    print(f'Looking at {table.table_id}')\n",
    "\n",
    "    # Test to see if table.table_id matches the pattern\n",
    "    # if so, delete it\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "129b9631",
   "metadata": {},
   "source": [
    "### Reading in and uploading montly tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3abf281",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here's a function to transform the date column in a dataframe to \n",
    "# the YYYYMM01 format we'd like to use for subsetting.\n",
    "\n",
    "def reformat_date(date_string) :\n",
    "    date_string = datetime.datetime.strptime(date_string,\"%Y-%m-%d\")\n",
    "    return(datetime.date.strftime(date_string,\"%Y%m\")+\"01\")\n",
    "\n",
    "assert(reformat_date(\"2022-09-20\")==\"20220901\")\n",
    "assert(reformat_date(\"2000-10-20\")==\"20001001\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a54dc09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this cell, do the following: \n",
    "# 1. Read in the items files one at a time.\n",
    "# 2. Do the same cleaning we did in Part 1 (clean names, \n",
    "#    make sku an empty string, fix dollars, make modifiers_applied a string)\n",
    "# 3. For each month in the file, subset the data to that month and \n",
    "#    upload the data to a table called `dram_items_YYYYMM01`. \n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "023eaf35",
   "metadata": {},
   "source": [
    "### Querying the Results\n",
    "\n",
    "In this section, write a query that returns the number of records per month across all of your tables and plot those results by month."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69dc3f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
